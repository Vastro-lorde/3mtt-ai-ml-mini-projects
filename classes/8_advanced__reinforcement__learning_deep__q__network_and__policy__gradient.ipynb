{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "944086ee",
      "metadata": {
        "id": "944086ee"
      },
      "source": [
        "ðŸŽ¥ Recommended Video: [Reinforcement Learning: Deep Q Learning and Policy Gradient](https://www.youtube.com/watch?v=k0eMEhgTYZQ&t=13s)\n",
        "\n",
        "\n",
        "\n",
        "## **1. Q-Learning**: Learning Through Trial and Error\n",
        "\n",
        "Imagine youâ€™re teaching a robot to navigate a simple maze. The robot knows nothing about the maze initially but learns through trial and error. As it moves, it receives rewards for getting closer to the goal and penalties for hitting walls. Over time, it develops a strategy to navigate efficiently. This learning process is the essence of **Q-Learning**.\n",
        "\n",
        "Q-Learning is a model-free reinforcement learning algorithm that helps an agent learn the value of taking specific actions in different states, using a **Q-table** to store these values.\n",
        "\n",
        "### Key Concepts:\n",
        "- **Q-table**: A grid where rows represent states, and columns represent actions. Each cell holds the expected reward (Q-value) for taking that action in that state.\n",
        "- **Bellman Equation**: Updates Q-values by considering both the immediate reward and the maximum future reward.\n",
        "\n",
        "$$\n",
        "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left( r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right)\n",
        "$$\n",
        "\n",
        "#### **Breaking Down the Terms:**\n",
        "- **$\\alpha$**: Learning rate, controlling how much new information overrides old knowledge.\n",
        "- **$\\gamma$**: Discount factor, balancing the importance of future rewards versus immediate rewards.\n",
        "- **$r$**: Immediate reward received for taking action $a$ in state $s$.\n",
        "- **$\\max_{a'} Q(s', a')$**: Maximum expected reward for the next state $s'$.\n",
        "\n",
        "#### Balancing Exploration and Exploitation\n",
        "Q-Learning uses an **epsilon-greedy strategy** to decide between:\n",
        "- **Exploration**: Trying new actions to discover better strategies.\n",
        "- **Exploitation**: Choosing the best-known action based on current knowledge.\n",
        "\n",
        "---\n",
        "\n",
        "### Example: Robot in a Simple Maze\n",
        "Letâ€™s simulate a robot learning to move through a maze using Q-Learning:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Define the environment\n",
        "num_states = 6\n",
        "num_actions = 2  # Example: 0 = left, 1 = right\n",
        "q_table = np.zeros((num_states, num_actions))\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.1  # Learning rate\n",
        "gamma = 0.9  # Discount factor\n",
        "epsilon = 1.0  # Exploration rate\n",
        "epsilon_decay = 0.995\n",
        "min_epsilon = 0.01\n",
        "\n",
        "# Simulate Q-Learning\n",
        "for episode in range(1000):\n",
        "    state = 0  # Start state\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        # Exploration vs Exploitation\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = np.random.randint(num_actions)  # Explore\n",
        "        else:\n",
        "            action = np.argmax(q_table[state])  # Exploit\n",
        "        \n",
        "        # Simulate action (example: move right or left)\n",
        "        if action == 1:  # Move right\n",
        "            next_state = state + 1\n",
        "        else:  # Move left\n",
        "            next_state = state - 1\n",
        "        \n",
        "        # Ensure next_state stays within valid bounds\n",
        "        next_state = np.clip(next_state, 0, num_states - 1)\n",
        "        \n",
        "        # Reward and done condition\n",
        "        reward = 1 if next_state == num_states - 1 else 0  # Reward at goal state\n",
        "        done = next_state == num_states - 1  # Episode ends at goal state\n",
        "        \n",
        "        # Update Q-value using Bellman Equation\n",
        "        q_table[state, action] += alpha * (reward + gamma * np.max(q_table[next_state]) - q_table[state, action])\n",
        "        \n",
        "        state = next_state\n",
        "    \n",
        "    # Decay epsilon\n",
        "    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
        "\n",
        "print(\"Q-Table:\")\n",
        "print(q_table)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the environment\n",
        "num_states = 6\n",
        "num_actions = 2  # Example: 0 = left, 1 = right\n",
        "q_table = np.zeros((num_states, num_actions))\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.1  # Learning rate\n",
        "gamma = 0.9  # Discount factor\n",
        "epsilon = 1.0  # Exploration rate\n",
        "epsilon_decay = 0.995\n",
        "min_epsilon = 0.01\n",
        "\n",
        "# Simulate Q-Learning\n",
        "for episode in range(1000):\n",
        "    state = 0  # Start state\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # Exploration vs Exploitation\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = np.random.randint(num_actions)  # Explore\n",
        "        else:\n",
        "            action = np.argmax(q_table[state])  # Exploit\n",
        "\n",
        "        # Simulate action (example: move right or left)\n",
        "        if action == 1:  # Move right\n",
        "            next_state = state + 1\n",
        "        else:  # Move left\n",
        "            next_state = state - 1\n",
        "\n",
        "        # Ensure next_state stays within valid bounds\n",
        "        next_state = np.clip(next_state, 0, num_states - 1)\n",
        "\n",
        "        # Reward and done condition\n",
        "        reward = 1 if next_state == num_states - 1 else 0  # Reward at goal state\n",
        "        done = next_state == num_states - 1  # Episode ends at goal state\n",
        "\n",
        "        # Update Q-value using Bellman Equation\n",
        "        q_table[state, action] += alpha * (reward + gamma * np.max(q_table[next_state]) - q_table[state, action])\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "    # Decay epsilon\n",
        "    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
        "\n",
        "print(\"Q-Table:\")\n",
        "print(q_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkjhiELIFdE4",
        "outputId": "01341152-c7be-4362-905d-aa8ff41e390c"
      },
      "id": "PkjhiELIFdE4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-Table:\n",
            "[[0.59048997 0.6561    ]\n",
            " [0.59048961 0.729     ]\n",
            " [0.65609989 0.81      ]\n",
            " [0.72899988 0.9       ]\n",
            " [0.80999924 1.        ]\n",
            " [0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2791581b",
      "metadata": {
        "id": "2791581b"
      },
      "source": [
        "## **2. Deep Q-Learning (DQN)**: Scaling to Complex Environments\n",
        "\n",
        "Now imagine the maze becomes vastly more complex, with thousands of states. A Q-table becomes impractical. Hereâ€™s where **Deep Q-Learning (DQN)** comes in, replacing the Q-table with a **neural network**(we'll look at neural networks in later modules)to approximate Q-values.\n",
        "\n",
        "### Key Concepts:\n",
        "- **Neural Network**: Learns to predict Q-values for each action given a state.\n",
        "- **Experience Replay**: Stores past experiences (state, action, reward, next state) in a buffer and samples them randomly to train the network. This reduces correlation in the data and improves stability.\n",
        "- **Target Network**: A separate network that provides stable target Q-values, updated less frequently than the main policy network.\n",
        "\n",
        "---\n",
        "\n",
        "### Example: Implementing DQN\n",
        "\n",
        "```python\n",
        "# Import deep learning libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "import random\n",
        "\n",
        "# Define the DQN network\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(DQN, self).__init__()\n",
        "        # Define a simple feed-forward network\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_size, 64),  # First hidden layer with 64 neurons\n",
        "            nn.ReLU(),                 # Activation function\n",
        "            nn.Linear(64, 64),         # Second hidden layer with 64 neurons\n",
        "            nn.ReLU(),                 # Activation function\n",
        "            nn.Linear(64, output_size) # Output layer matching the action space size\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Forward pass through the network\n",
        "        return self.network(x)\n",
        "\n",
        "# Experience Replay Buffer\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity=10000):\n",
        "        self.buffer = deque(maxlen=capacity)  # Stores a fixed number of experiences\n",
        "    \n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        # Add a new experience to the buffer\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        # Randomly sample a batch of experiences\n",
        "        return random.sample(self.buffer, batch_size)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.buffer)  # Return the current size of the buffer\n",
        "\n",
        "# DQN Agent\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available\n",
        "        \n",
        "        # Initialize the policy network and target network\n",
        "        self.policy_net = DQN(state_size, action_size).to(self.device)\n",
        "        self.target_net = DQN(state_size, action_size).to(self.device)\n",
        "        \n",
        "        # Copy weights from the policy network to the target network\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        \n",
        "        # Optimizer and replay buffer\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters())\n",
        "        self.memory = ReplayBuffer()\n",
        "        \n",
        "        # Training hyperparameters\n",
        "        self.batch_size = 32\n",
        "        self.gamma = 0.99  # Discount factor\n",
        "        self.epsilon = 0.1  # Exploration rate\n",
        "        \n",
        "    def select_action(self, state):\n",
        "        # Choose an action using epsilon-greedy policy\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randrange(self.policy_net.network[-1].out_features)  # Explore\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            state = torch.FloatTensor(state).unsqueeze(0).to(self.device)  # Prepare state tensor\n",
        "            return self.policy_net(state).max(1)[1].item()  # Exploit\n",
        "    \n",
        "    def learn(self):\n",
        "        # Skip learning if not enough experiences in memory\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "        \n",
        "        # Sample a batch of experiences\n",
        "        batch = self.memory.sample(self.batch_size)\n",
        "        batch_state, batch_action, batch_reward, batch_next_state, batch_done = zip(*batch)\n",
        "        \n",
        "        # Convert batch data to tensors\n",
        "        state = torch.FloatTensor(batch_state).to(self.device)\n",
        "        action = torch.LongTensor(batch_action).to(self.device)\n",
        "        reward = torch.FloatTensor(batch_reward).to(self.device)\n",
        "        next_state = torch.FloatTensor(batch_next_state).to(self.device)\n",
        "        done = torch.FloatTensor(batch_done).to(self.device)\n",
        "        \n",
        "        # Compute current Q-values for chosen actions\n",
        "        current_q = self.policy_net(state).gather(1, action.unsqueeze(1))\n",
        "        \n",
        "        # Compute next Q-values using the target network\n",
        "        next_q = self.target_net(next_state).max(1)[0].detach()\n",
        "        \n",
        "        # Compute the target Q-value using the Bellman equation\n",
        "        target_q = reward + (1 - done) * self.gamma * next_q\n",
        "        \n",
        "        # Calculate the loss between current and target Q-values\n",
        "        loss = nn.MSELoss()(current_q.squeeze(), target_q)\n",
        "        \n",
        "        # Perform backpropagation\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step())\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "By evolving from Q-Learning to Deep Q-Learning, we unlock the ability to handle more complex environments, setting the stage for powerful reinforcement learning applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "deb35a73",
      "metadata": {
        "id": "deb35a73"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}