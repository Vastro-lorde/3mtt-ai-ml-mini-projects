{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1e82475c",
      "metadata": {
        "id": "1e82475c"
      },
      "source": [
        "ðŸŽ¥ Recommended Video: [Large Language Models explained briefly](https://www.youtube.com/watch?v=LPZh9BOjkQs&t=17s)\n",
        "\n",
        "\n",
        "### **8.7 Large Language Models (LLMs) as Generative Models**\n",
        "\n",
        "#### **What are LLMs?**\n",
        "Large Language Models are a type of neural network trained on vast amounts of text data to understand and generate human-like text. They are based on architectures like **Transformers**, which excel at capturing long-range dependencies in sequential data (e.g., sentences, paragraphs).\n",
        "\n",
        "#### **Why are LLMs Generative Models?**\n",
        "LLMs are generative because they:\n",
        "1. **Learn the distribution of language**: They model the probability distribution of words, sentences, or sequences in a given dataset.\n",
        "2. **Generate new text**: They can produce coherent and contextually relevant text based on a prompt or input.\n",
        "3. **Create diverse outputs**: They can generate multiple plausible responses for the same input, showcasing their generative nature.\n",
        "\n",
        "#### **Key Features of LLMs**:\n",
        "- **Autoregressive Generation**: LLMs generate text one token (word or subword) at a time, using previously generated tokens as context.\n",
        "- **Conditional Generation**: They can generate text conditioned on a specific input (e.g., answering a question, completing a sentence).\n",
        "- **Fine-Tuning**: LLMs can be fine-tuned for specific tasks like summarization, translation, or dialogue generation.\n",
        "\n",
        "---\n",
        "\n",
        "### **8.8 How LLMs Work**\n",
        "LLMs are typically based on the **Transformer architecture**, which uses self-attention mechanisms to process input sequences. Hereâ€™s a high-level overview of how they generate text:\n",
        "\n",
        "1. **Input Encoding**: The input text is tokenized and converted into embeddings (vector representations).\n",
        "2. **Self-Attention**: The model computes attention scores to understand relationships between words in the input.\n",
        "3. **Decoding**: The model generates text autoregressively, predicting the next token based on the context of previously generated tokens.\n",
        "4. **Output**: The generated tokens are converted back into human-readable text.\n",
        "\n",
        "---\n",
        "\n",
        "### **8.9 Code Example: Using a Pre-Trained LLM**\n",
        "Letâ€™s use the Hugging Face `transformers` library to generate text with a pre-trained LLM like GPT-2.\n",
        "\n",
        "```python\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load pre-trained GPT-2 model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Encode input text\n",
        "input_text = \"Once upon a time\"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "# Generate text\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    max_length=50,  # Maximum length of generated text\n",
        "    num_return_sequences=1,  # Number of sequences to generate\n",
        "    no_repeat_ngram_size=2,  # Avoid repeating n-grams\n",
        "    top_k=50,  # Top-k sampling\n",
        "    top_p=0.95,  # Nucleus sampling\n",
        "    temperature=0.7  # Controls randomness\n",
        ")\n",
        "\n",
        "# Decode and print the generated text\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(generated_text)\n",
        "```\n",
        "\n",
        "#### **Explanation**:\n",
        "1. The GPT-2 model is loaded and used to generate text based on the input prompt.\n",
        "2. Parameters like `max_length`, `top_k`, and `temperature` control the behavior of the text generation process.\n",
        "3. The output is a coherent continuation of the input text.\n",
        "\n",
        "---\n",
        "\n",
        "### **8.10 Applications of LLMs**\n",
        "LLMs are used in a wide range of applications, including:\n",
        "- **Text Generation**: Writing stories, articles, or code.\n",
        "- **Chatbots**: Powering conversational agents like ChatGPT.\n",
        "- **Summarization**: Condensing long documents into shorter summaries.\n",
        "- **Translation**: Translating text between languages.\n",
        "- **Question Answering**: Providing answers to user queries.\n",
        "\n",
        "---\n",
        "\n",
        "### **8.11 Key Takeaways**\n",
        "- LLMs are a type of generative model focused on text data.\n",
        "- They use architectures like Transformers to model and generate human-like text.\n",
        "- LLMs have a wide range of applications in natural language processing (NLP).\n",
        "\n",
        "---\n",
        "\n",
        "### **8.12 Comparison of Generative Models**\n",
        "| **Model Type**       | **Data Type** | **Key Features**                                                                 |\n",
        "|-----------------------|---------------|----------------------------------------------------------------------------------|\n",
        "| **GANs**             | Images        | Generates realistic images through adversarial training.                         |\n",
        "| **VAEs**             | Images        | Learns a probabilistic latent space for generating diverse outputs.              |\n",
        "| **Diffusion Models** | Images        | Gradually denoises data to generate high-quality samples.                        |\n",
        "| **LLMs**             | Text          | Generates coherent and contextually relevant text using autoregressive methods.  |\n",
        "\n",
        "---\n",
        "LLMs are a powerful class of generative models specifically designed for text data. They share the core idea of learning data distributions and generating new samples, just like GANs, VAEs, and diffusion models do for images. If you'd like to dive deeper into LLMs or explore specific use cases, let me know!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1415993e",
      "metadata": {
        "id": "1415993e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}