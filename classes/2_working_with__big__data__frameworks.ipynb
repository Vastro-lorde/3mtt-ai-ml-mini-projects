{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaf29827",
   "metadata": {},
   "source": [
    "\n",
    "ðŸŽ¥ Recommended Video: [The ONLY PySpark Tutorial You Will Ever Need](https://www.youtube.com/watch?v=LHXXI4-IEns)\n",
    "\n",
    "ðŸŽ¥ Recommended Video: [Dask in 15 Minutesd](https://www.youtube.com/watch?v=Alwgx_1qsj4)\n",
    "\n",
    "### **Introduction: The Big Data Adventure**\n",
    "\n",
    "Imagine youâ€™re standing at the edge of a vast, uncharted ocean. This ocean isnâ€™t made of waterâ€”itâ€™s made of data. Every wave is a transaction, every ripple a customer interaction, and every current a trend waiting to be discovered. But hereâ€™s the catch: the ocean is so enormous that no ordinary tool can help you navigate it. You need something powerful, something scalable, something that can turn this overwhelming sea of information into actionable insights.\n",
    "\n",
    "Enter **Big Data Frameworks**â€”your trusty ships and compasses in this data-driven voyage. Today, weâ€™re setting sail with two of the most popular tools in the big data ecosystem: **PySpark** and **Dask**. These frameworks are like the captains of your fleet, each with its own strengths and specialties. \n",
    "\n",
    "- **PySpark** is the seasoned explorer, built for speed and scalability. Itâ€™s the go-to choice when youâ€™re dealing with massive datasets that need to be processed across clusters of machines. Think of it as your battleship, ready to tackle the toughest data challenges with ease.\n",
    "  \n",
    "- **Dask**, on the other hand, is the agile scout. Itâ€™s lightweight, flexible, and perfect for when you need to scale up your Python workflows without the overhead of a full-blown distributed system. Itâ€™s like a nimble sailboat, ideal for smaller expeditions or when youâ€™re just starting to dip your toes into the big data waters.\n",
    "\n",
    "In this lecture, weâ€™ll dive into the world of big data processing. Youâ€™ll learn how to use PySpark to analyze sales data across regions and how Dask can help you process large CSV files that would make Pandas sweat. By the end of this journey, youâ€™ll have the tools and knowledge to navigate the data ocean with confidence, uncovering insights that can transform your business or research.\n",
    "\n",
    "So, grab your compass, hoist the sails, and letâ€™s embark on this big data adventure together! ðŸŒŠðŸš€\n",
    "\n",
    "\n",
    "\n",
    "### **PySpark**\n",
    "- **What it does**: PySpark is the Python library for Spark. It lets you write Spark code in Python, making it easier for data scientists.\n",
    "- **Why use PySpark?**: Itâ€™s fast, scalable, and integrates well with Pythonâ€™s data science ecosystem.\n",
    "\n",
    "#### **Example: Analyzing Sales Data**\n",
    "Letâ€™s say you have a huge dataset of sales transactions. You can use PySpark to calculate total sales per region.\n",
    "\n",
    "```python\n",
    "# PySpark example: Sales Analysis\n",
    "sales = spark.read.csv(\"sales.csv\", header=True, inferSchema=True)\n",
    "total_sales = sales.groupBy(\"region\").sum(\"amount\")\n",
    "total_sales.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Dask**\n",
    "- **What it does**: Dask is like Sparkâ€™s younger sibling. Itâ€™s designed to work with Python libraries like Pandas and NumPy but scales to big data.\n",
    "- **Why use Dask?**: Itâ€™s lightweight and easy to use for smaller clusters or personal computers.\n",
    "\n",
    "#### **Example: Processing Large CSV Files**\n",
    "If you have a CSV file too big for Pandas, Dask can handle it.\n",
    "\n",
    "```python\n",
    "# Dask example: Large CSV Processing\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Load large CSV\n",
    "df = dd.read_csv(\"large_data.csv\")\n",
    "\n",
    "# Perform operations\n",
    "total_sales = df.groupby(\"region\").amount.sum().compute()\n",
    "print(total_sales)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "CHeck out this notebook for more use cases of pySpark and Dask. ðŸ‘‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf30065",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
